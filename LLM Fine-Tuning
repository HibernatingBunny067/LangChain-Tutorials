{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6ecfc8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-26T18:36:33.325571Z",
     "iopub.status.busy": "2026-01-26T18:36:33.324793Z",
     "iopub.status.idle": "2026-01-26T18:36:52.078055Z",
     "shell.execute_reply": "2026-01-26T18:36:52.076983Z"
    },
    "papermill": {
     "duration": 18.759537,
     "end_time": "2026-01-26T18:36:52.080584",
     "exception": false,
     "start_time": "2026-01-26T18:36:33.321047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121cd8e",
   "metadata": {
    "papermill": {
     "duration": 0.001327,
     "end_time": "2026-01-26T18:36:52.083570",
     "exception": false,
     "start_time": "2026-01-26T18:36:52.082243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Fine-Tuning and how is this different from RAG ?\n",
    "- specializing a LLM for a particular task by training on a small set of data\n",
    "- this is done to excel at a particular task, intuitively we are tweaking the brain connections (weights) of the model to learn something particular out of it's training data\n",
    "- finetuning is no magical technique, it's used to make a model more opionated rather than general\n",
    "- Finetuning comes with the risk of catastrophic forgetting (very high lr with low data)\n",
    "- thinking it like a MBBS student which goes on to do specialization is a loose metaphor because the human their has the ability to distinguish between spurious correlations but models in training don't or rather humans have external and internal mechanism to identify and suppress spurious correlations where models treate any loss reducing pattern as the legitimate unless explicity constrained !!\n",
    "\n",
    "### EX: we may take a small llama 2b or 1b param model and fine tune it on say Legal Datasets, we might be able to get a model which not only excels at answering naturally but also has a specialized knowledge in Legal cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08dfe4",
   "metadata": {
    "papermill": {
     "duration": 0.001295,
     "end_time": "2026-01-26T18:36:52.086130",
     "exception": false,
     "start_time": "2026-01-26T18:36:52.084835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quantization\n",
    "**1. Symmetric Quantization: real values are roughly centered around zero**\n",
    "- zero_point = 00\n",
    "- equal ranges on both the sides\n",
    "- $x_{int} = cilp(round(\\frac{x_{float}}{s}),-Q_{max},Q_{max})$\n",
    "\n",
    "**2. Asymmetric Quantization**\n",
    "\n",
    "- when the assumptions of symmetric quantization fail (usually in activations)\n",
    "- $x_{int} = round(\\frac{x_{float}}{s} + z)$\n",
    "- where s is the scale and z is the zero point\n",
    "\n",
    "## Post Training Quantization\n",
    "- we start with a pre-trained model (typically in FP32) -> Calibrate the weights to the desired size -> Quantized model\n",
    "\n",
    "## Quantization aware training\n",
    "- forward pass uses quantized values -> backward pass updates FP weights -> model learns that tiny values get killed\n",
    "\n",
    "### Collapse in Quantization \n",
    "- INT8 quantization handles value collapse not by preventing it, but by reshaping distributions (via scaling, per-channel quantization, clipping, and QAT) so that important distinctions survive bucketization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00db59d",
   "metadata": {
    "papermill": {
     "duration": 0.00122,
     "end_time": "2026-01-26T18:36:52.088522",
     "exception": false,
     "start_time": "2026-01-26T18:36:52.087302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.485092,
   "end_time": "2026-01-26T18:36:55.444454",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-26T18:36:29.959362",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
